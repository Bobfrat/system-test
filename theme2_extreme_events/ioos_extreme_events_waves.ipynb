{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "># IOOS System Test: [Extreme Events Theme:](https://github.com/ioos/system-test/wiki/Development-of-Test-Themes#theme-2-extreme-events) Inundation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Can we estimate the return period of a wave height by comparing modeled and/or observed wave heights with NOAA exceedance probability plots?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "import required libraries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import matplotlib.pyplot as plt\n",
      "from pylab import *\n",
      "import sys\n",
      "import csv\n",
      "import json\n",
      "from scipy.stats import genextreme\n",
      "import scipy.stats as ss\n",
      "import numpy as np\n",
      "\n",
      "from owslib.csw import CatalogueServiceWeb\n",
      "from owslib import fes\n",
      "import random\n",
      "import netCDF4\n",
      "import pandas as pd\n",
      "import datetime as dt\n",
      "from pyoos.collectors.ndbc.ndbc_sos import NdbcSos\n",
      "import cStringIO\n",
      "import iris\n",
      "import urllib2\n",
      "import parser\n",
      "from lxml import etree       #TODO suggest using bs4 instead for ease of access to XML objects\n",
      "\n",
      "#generated for csw interface\n",
      "#from date_range_formatter import dateRange  #date formatter (R.Signell)\n",
      "import requests              #required for the processing of requests\n",
      "from bs4 import *            #required for the xml parsing of getcaps and get obs\n",
      "from signell_search_functions import * \n",
      "\n",
      "from IPython.display import HTML\n",
      "import folium #required for leaflet mapping\n",
      "from map_bb import get_coordinates # required for mapping the coordinates\n",
      "import calendar #used to get number of days in a month and year"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING: pylab import has clobbered these variables: ['power', 'axes', 'box', 'xlabel', 'linalg', 'random', 'title', 'Polygon', 'poly', 'fft', 'info']\n",
        "`%pylab --no-import-all` prevents importing * from pylab and numpy\n"
       ]
      }
     ],
     "prompt_number": 168
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "some functions from [Rich Signell Notebook](http://nbviewer.ipython.org/github/rsignell-usgs/notebook/blob/fef9438303b49a923024892db1ef3115e34d8271/CSW/IOOS_inundation.ipynb)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Speficy Temporal and Spatial conditions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#bounding box of interest,[bottom right[lat,lon], top left[lat,lon]]\n",
      "bounding_box_type = \"box\" \n",
      "bounding_box = [[-73.94,40.67],[-69.94,42]]\n",
      "\n",
      "#temporal range\n",
      "start_date = dt.datetime(1991,5,1).strftime('%Y-%m-%d %H:00')\n",
      "end_date = dt.datetime(2014,5,10).strftime('%Y-%m-%d %H:00')\n",
      "time_date_range = [start_date,end_date]  #start_date_end_date\n",
      "\n",
      "print start_date,'to',end_date"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1991-05-01 00:00 to 2014-05-10 00:00\n"
       ]
      }
     ],
     "prompt_number": 169
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "endpoint = 'http://www.ngdc.noaa.gov/geoportal/csw' # NGDC Geoportal\n",
      "csw = CatalogueServiceWeb(endpoint,timeout=60)\n",
      "\n",
      "for oper in csw.operations:\n",
      "    if oper.name == 'GetRecords':\n",
      "        #print '\\nISO Queryables:\\n',oper.constraints['SupportedISOQueryables']['values']\n",
      "        pass\n",
      "        \n",
      "#put the names in a dict for ease of access \n",
      "data_dict = {}\n",
      "data_dict[\"waves\"] = {\"names\":['sea_surface_wave_significant_height','significant_wave_height'], \n",
      "                      \"sos_name\":[\"waves\"]}      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 170
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dateRange(start_date='1900-01-01',stop_date='2100-01-01',constraint='overlaps'):\n",
      "    if constraint == 'overlaps':\n",
      "        start = fes.PropertyIsLessThanOrEqualTo(propertyname='apiso:TempExtent_begin', literal=stop_date)\n",
      "        stop = fes.PropertyIsGreaterThanOrEqualTo(propertyname='apiso:TempExtent_end', literal=start_date)\n",
      "    elif constraint == 'within':\n",
      "        start = fes.PropertyIsGreaterThanOrEqualTo(propertyname='apiso:TempExtent_begin', literal=start_date)\n",
      "        stop = fes.PropertyIsLessThanOrEqualTo(propertyname='apiso:TempExtent_end', literal=stop_date)\n",
      "    return start,stop"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 171
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert User Input into FES filters\n",
      "start,stop = dateRange(start_date,end_date)\n",
      "box = []\n",
      "box.append(bounding_box[0][0])\n",
      "box.append(bounding_box[0][1])\n",
      "box.append(bounding_box[1][0])\n",
      "box.append(bounding_box[1][1])\n",
      "bbox = fes.BBox(box)\n",
      "\n",
      "#use the search name to create search filter\n",
      "or_filt = fes.Or([fes.PropertyIsLike(propertyname='apiso:AnyText',literal=('*%s*' % val),\n",
      "                    escapeChar='\\\\',wildCard='*',singleChar='?') for val in data_dict[\"waves\"][\"names\"]])\n",
      "val = 'Averages'\n",
      "not_filt = fes.Not([fes.PropertyIsLike(propertyname='apiso:AnyText',literal=('*%s*' % val),\n",
      "                        escapeChar='\\\\',wildCard='*',singleChar='?')])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 172
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filter_list = [fes.And([ bbox, start, stop, or_filt, not_filt]) ]\n",
      "# connect to CSW, explore it's properties\n",
      "# try request using multiple filters \"and\" syntax: [[filter1,filter2]]\n",
      "csw.getrecords2(constraints=filter_list,maxrecords=1000,esn='full')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 173
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def service_urls(records,service_string='urn:x-esri:specification:ServiceType:odp:url'):\n",
      "    \"\"\"\n",
      "    extract service_urls of a specific type (DAP, SOS) from records\n",
      "    \"\"\"\n",
      "    urls=[]\n",
      "    for key,rec in records.iteritems():        \n",
      "        #create a generator object, and iterate through it until the match is found\n",
      "        #if not found, gets the default value (here \"none\")\n",
      "        url = next((d['url'] for d in rec.references if d['scheme'] == service_string), None)\n",
      "        if url is not None:\n",
      "            urls.append(url)\n",
      "    return urls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 174
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print records that are available\n",
      "print \"number of datasets available: \",len(csw.records.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of datasets available:  12\n"
       ]
      }
     ],
     "prompt_number": 175
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Print all the records (should you want too)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"\\n\".join(csw.records)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "WW3/Forecasts/GulfOfMaine.nc\n",
        "necofs_gom3_wave\n",
        "WW3/Forecasts/EastCoast.nc\n",
        "inundation_tropical.VIMS_SELFE.Hurricane_Ike_2D_final_run_wave_only\n",
        "inundation_tropical.VIMS_SELFE.Hurricane_Ike_3D_final_run_with_waves\n",
        "inundation_tropical.VIMS_SELFE.Hurricane_Rita_2D_final_run_waves_only\n",
        "inundation_tropical.VIMS_SELFE.Hurricane_Rita_2D_final_run_with_waves\n",
        "inundation_tropical.VIMS_SELFE.Hurricane_Rita_3D_final_run_with_waves\n",
        "coawst_4/use/fmrc/coawst_4_use_best.ncd\n",
        "WW3/Forecasts/NorthAtlantic.nc\n",
        "inundation_tropical.VIMS_SELFE.Hurricane_Ike_2D_final_run_with_waves\n",
        "ww3_global\n"
       ]
      }
     ],
     "prompt_number": 176
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dap URLS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dap_urls = service_urls(csw.records,service_string='urn:x-esri:specification:ServiceType:odp:url')\n",
      "#remove duplicates and organize\n",
      "dap_urls = sorted(set(dap_urls))\n",
      "print \"Total DAP:\",len(dap_urls)\n",
      "#print the first 5...\n",
      "print \"\\n\".join(dap_urls[:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total DAP: 12\n",
        "http://comt.sura.org/thredds/dodsC/data/comt_1_archive/inundation_tropical/VIMS_SELFE/Hurricane_Ike_2D_final_run_wave_only\n",
        "http://comt.sura.org/thredds/dodsC/data/comt_1_archive/inundation_tropical/VIMS_SELFE/Hurricane_Ike_2D_final_run_with_waves\n",
        "http://comt.sura.org/thredds/dodsC/data/comt_1_archive/inundation_tropical/VIMS_SELFE/Hurricane_Ike_3D_final_run_with_waves\n",
        "http://comt.sura.org/thredds/dodsC/data/comt_1_archive/inundation_tropical/VIMS_SELFE/Hurricane_Rita_2D_final_run_waves_only\n",
        "http://comt.sura.org/thredds/dodsC/data/comt_1_archive/inundation_tropical/VIMS_SELFE/Hurricane_Rita_2D_final_run_with_waves\n",
        "http://comt.sura.org/thredds/dodsC/data/comt_1_archive/inundation_tropical/VIMS_SELFE/Hurricane_Rita_3D_final_run_with_waves\n",
        "http://geoport.whoi.edu/thredds/dodsC/coawst_4/use/fmrc/coawst_4_use_best.ncd\n",
        "http://oos.soest.hawaii.edu/thredds/dodsC/hioos/model/wav/ww3/WaveWatch_III_Global_Wave_Model_best.ncd\n",
        "http://www.neracoos.org/thredds/dodsC/WW3/EastCoast.nc\n",
        "http://www.neracoos.org/thredds/dodsC/WW3/GulfOfMaine.nc\n",
        "http://www.neracoos.org/thredds/dodsC/WW3/NorthAtlantic.nc\n",
        "http://www.smast.umassd.edu:8080/thredds/dodsC/FVCOM/NECOFS/Forecasts/NECOFS_WAVE_FORECAST.nc\n"
       ]
      }
     ],
     "prompt_number": 177
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SOS URLs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### TODO: Fix waves not being found in catalog"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sos_urls = service_urls(csw.records,service_string='urn:x-esri:specification:ServiceType:sos:url')\n",
      "#remove duplicates and organize\n",
      "if len(sos_urls) ==0:\n",
      "    sos_urls.append(\"http://sdf.ndbc.noaa.gov/sos/server.php\")  #?request=GetCapabilities&service=SOS\n",
      "\n",
      "sos_urls = sorted(set(sos_urls))\n",
      "print \"Total SOS:\",len(sos_urls)\n",
      "print \"\\n\".join(sos_urls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total SOS: 1\n",
        "http://sdf.ndbc.noaa.gov/sos/server.php\n"
       ]
      }
     ],
     "prompt_number": 178
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### SOS Requirements"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#use the get caps to get station start and get time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 179
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_time = dt.datetime.strptime(start_date,'%Y-%m-%d %H:%M')\n",
      "end_time = dt.datetime.strptime(end_date,'%Y-%m-%d %H:%M')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 180
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iso_start = start_time.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
      "iso_end = end_time.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
      "\n",
      "collector = NdbcSos()\n",
      "collector.start_time = start_time\n",
      "collector.end_time = end_time\n",
      "collector.variables = data_dict[\"waves\"][\"sos_name\"]\n",
      "collector.server.identification.title\n",
      "print collector.start_time,\":\", collector.end_time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1991-05-01 00:00:00+00:00 : 2014-05-10 00:00:00+00:00\n"
       ]
      }
     ],
     "prompt_number": 181
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Date: \",iso_start,\" to \", iso_end\n",
      "box_str=','.join(str(e) for e in box)\n",
      "print \"Lat/Lon Box: \",box_str\n",
      "#grab the sos url and use it for the service\n",
      "url=(sos_urls[0]+'?'\n",
      "     'service=SOS&request=GetObservation&version=1.0.0&'\n",
      "     'observedProperty=%s&offering=urn:ioos:network:noaa.nws.ndbc:all&'\n",
      "     'featureOfInterest=BBOX:%s&responseFormat=text/tab-separated-values&eventTime=%s') % (\"waves\",box_str,iso_end)\n",
      "print url\n",
      "r = requests.get(url)\n",
      "data = r.text\n",
      "#get the headers for the cols\n",
      "data = data.split(\"\\n\")\n",
      "headers =  data[0]\n",
      "station_list_dict = dict()\n",
      "#parse the headers so i can create a dict\n",
      "c = 0\n",
      "for h in headers.split(\"\\t\"):\n",
      "    field = h.split(\":\")[0].split(\" \")[0]\n",
      "    station_list_dict[field] = {\"id\":c}\n",
      "    c+=1\n",
      "print \"Num of fields:\", c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Date:  1991-05-01T00:00:00Z  to  2014-05-10T00:00:00Z\n",
        "Lat/Lon Box:  -73.94,40.67,-69.94,42\n",
        "http://sdf.ndbc.noaa.gov/sos/server.php?service=SOS&request=GetObservation&version=1.0.0&observedProperty=waves&offering=urn:ioos:network:noaa.nws.ndbc:all&featureOfInterest=BBOX:-73.94,40.67,-69.94,42&responseFormat=text/tab-separated-values&eventTime=2014-05-10T00:00:00Z\n",
        "Num of fields:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 26\n"
       ]
      }
     ],
     "prompt_number": 192
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create dict of stations\n",
      "station_list = []\n",
      "for i in range(1,len(data)):\n",
      "    station_info = data[i].split(\"\\t\")\n",
      "    if len(station_info)>1:\n",
      "        station = dict()\n",
      "        for field in station_list_dict.keys():        \n",
      "            col = station_list_dict[field][\"id\"]\n",
      "            if col < len(station_info):\n",
      "                station[field] = station_info[col]     \n",
      "        station[\"type\"] = \"obs\"        \n",
      "        station_list.append(station)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 250
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Add wis site infotmation to station list"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(station_list)    \n",
      "print float(station_list[0][\"longitude\"])\n",
      "print station_list[0][\"latitude\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "-72.655\n",
        "41.138\n"
       ]
      }
     ],
     "prompt_number": 251
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from shapely.geometry import Polygon,Point,LineString\n",
      "import sqlite3 as lite"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 252
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the WIS stations that are in the bounding box\n",
      "#generate polygon that matches that of the bounding box\n",
      "poly = Polygon(get_coordinates(bounding_box,bounding_box_type))\n",
      "db = r'/Users/rpsdev/Documents/d3/wis_data/wis_stations.db'\n",
      "print poly\n",
      "try:\n",
      "    conn = lite.connect(db)\n",
      "    cur = conn.cursor()\n",
      "    cur.execute(\"SELECT NAME,LAT,LON FROM station_list\")\n",
      "    records = cur.fetchall()\n",
      "    st_yr =int(collector.start_time.year)\n",
      "    ed_yr =int(collector.end_time.year+1)\n",
      "    min_station_line = -1\n",
      "    min_station_line_set = True\n",
      "    min_station_name = \"\"\n",
      "    station_lat = float(station_list[0][\"latitude\"])\n",
      "    station_lon = float(station_list[0][\"longitude\"])\n",
      "    for r in records:\n",
      "        #name,lat,lon\n",
      "        station_is_contained = poly.contains(Point(float(r[1]),float(r[2])))\n",
      "        #if the station is in the bounds, store it as such\n",
      "        if station_is_contained:\n",
      "            st_name = r[0]\n",
      "            #find the closest station\n",
      "            \n",
      "            line = LineString([(float(r[1]),float(r[2])),(station_lat,station_lon)])\n",
      "            if min_station_line_set:\n",
      "                min_station_line = line.length\n",
      "                min_station_record = r\n",
      "            if line.length < min_station_line:\n",
      "                min_station_record = r\n",
      "                \n",
      "    station = dict()\n",
      "    station[\"latitude\"] = float(min_station_record[1])\n",
      "    station[\"longitude\"] = float(min_station_record[2])\n",
      "    st_name = min_station_record[0]\n",
      "    station[\"long_name\"] = \"WIS:\"+ st_name\n",
      "    station[\"id\"] = st_name\n",
      "    station[\"station_id\"] = st_name\n",
      "    station[\"type\"] = \"model\"\n",
      "    station_data = dict()\n",
      "    for yr in range(st_yr,ed_yr+1):\n",
      "        #get the max value from the monthly max to give yearly max\n",
      "        cur.execute(\"select MAX(hmax) from station_data where name=\"+ str(st_name) +\" and date >=\"+str(yr)+\"01 and date <=\"+str(yr)+\"12\")\n",
      "        yearmax = cur.fetchall()\n",
      "        yearmax = yearmax[0][0]\n",
      "        if yearmax is None:\n",
      "            print \"year \", yr,\" is none\"\n",
      "        else:\n",
      "            station_data[str(yr)] = {\"max\":yearmax,\"num_samples\":0,\"date_string\":\"\"}\n",
      "    station[\"data\"] = station_data\n",
      "    station_list.append(station)\n",
      "except lite.Error:\n",
      "    print \"Error open db.\\n\"\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "POLYGON ((40.67 -73.94, 40.67 -69.94, 42 -69.94, 42 -73.94, 40.67 -73.94))\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2001  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2002  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2003  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2004  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2005  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2006  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2007  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2008  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2009  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2010  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2011  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2012  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2013  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2014  is none\n",
        "year "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2015  is none\n"
       ]
      }
     ],
     "prompt_number": 253
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print out the station name\n",
      "print station_list[0][\"sensor_id\"]\n",
      "print station_list[1]\n",
      "print len(station_list) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "urn:ioos:sensor:wmo:44039::summarywav1\n",
        "{'data': {'1991': {'max': 5.2, 'date_string': '', 'num_samples': 0}, '1993': {'max': 6.3, 'date_string': '', 'num_samples': 0}, '1992': {'max': 8.13, 'date_string': '', 'num_samples': 0}, '1995': {'max': 5.81, 'date_string': '', 'num_samples': 0}, '1994': {'max': 5.79, 'date_string': '', 'num_samples': 0}, '1997': {'max': 5.9, 'date_string': '', 'num_samples': 0}, '1996': {'max': 5.22, 'date_string': '', 'num_samples': 0}, '1999': {'max': 5.48, 'date_string': '', 'num_samples': 0}, '1998': {'max': 4.79, 'date_string': '', 'num_samples': 0}}, 'station_id': u'63108', 'long_name': u'WIS:63108', 'longitude': -72.33, 'latitude': 40.75, 'type': 'model', 'id': u'63108'}\n",
        "2\n"
       ]
      }
     ],
     "prompt_number": 254
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_station_long_name(sta):\n",
      "    \"\"\"\n",
      "    get longName for specific station\n",
      "    \"\"\"\n",
      "    url=(sos_urls[0]+'?service=SOS&'\n",
      "        'request=DescribeSensor&version=1.0.0&outputFormat=text/xml;subtype=\"sensorML/1.0.1\"&'\n",
      "        'procedure=%s') % sta    \n",
      "    tree = etree.parse(urllib2.urlopen(url))\n",
      "    root = tree.getroot()\n",
      "    longName=root.xpath(\"//sml:identifier[@name='longName']/sml:Term/sml:value/text()\", namespaces={'sml':\"http://www.opengis.net/sensorML/1.0.1\"})\n",
      "    return longName"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 255
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_sos_data(collector,station_id,sos_name,date_time,field_of_interest):\n",
      "    print \"Station:\",station_id\n",
      "    collector.features = [station_id]\n",
      "    collector.variables = [sos_name] \n",
      "    station_data = dict()\n",
      "    #loop through the years and get the data needed\n",
      "    st_yr =int(collector.start_time.year)\n",
      "    ed_yr =int(collector.end_time.year+1)\n",
      "    #only 31 days are allowed to be requested at once\n",
      "    for year_station in range(st_yr,ed_yr):    \n",
      "        year_station_data = []\n",
      "        date_list = []\n",
      "        for month in range (1,13):\n",
      "                num_days = calendar.monthrange(year_station, month)[1]     \n",
      "\n",
      "                st = dt.datetime(year_station,month,1,0,0,0)\n",
      "                ed = dt.datetime(year_station,month,num_days,23,59,59)\n",
      "\n",
      "                start_time1 = dt.datetime.strptime(str(st),'%Y-%m-%d %H:%M:%S')\n",
      "                end_time1 = dt.datetime.strptime(str(ed),'%Y-%m-%d %H:%M:%S')\n",
      "                \n",
      "                collector.start_time = start_time1\n",
      "                collector.end_time = end_time1\n",
      "                 \n",
      "                try:\n",
      "                    response = collector.raw(responseFormat=\"text/csv\")\n",
      "                    #get the response then get the data\n",
      "                    data =  response.split(\"\\n\")\n",
      "                    first_row = True\n",
      "                    if len(data)>2:\n",
      "                        for d in data:\n",
      "                            d_row = (d.split(\",\"))\n",
      "                            \n",
      "                            if first_row:\n",
      "                                #find the field of interest\n",
      "                                idx1 = [d_row.index(i) for i in d_row if field_of_interest in i][0]\n",
      "                                idx2 = [d_row.index(i) for i in d_row if date_time in i][0]\n",
      "                                first_row = False\n",
      "                            else:  \n",
      "                                if idx1<len(d_row):\n",
      "                                    year_station_data.append(d_row[idx1])\n",
      "                                else:\n",
      "                                    #print idx1,\":\",d_row\n",
      "                                    pass\n",
      "                                    \n",
      "                                if idx2<len(d_row):\n",
      "                                    date_list.append(d_row[idx2])\n",
      "                                else:\n",
      "                                    #print idx2,\":\",d_row\n",
      "                                    pass\n",
      "    \n",
      "                    else:\n",
      "                        #print \"no data...:\",year_station,\":\",month\n",
      "                        pass\n",
      "                except Exception, e: #should only fail if the data is not there\n",
      "                    print e,\":\",year_station,\":\",month\n",
      "\n",
      "        #caluclate the max values   \n",
      "        p = np.array(year_station_data,dtype=np.float)     \n",
      "        if len(p)>2:\n",
      "            station_data[str(year_station)] = {\"max\":np.amax(p),\"num_samples\":len(p),\"date_string\":date_list[np.argmax(p)]}\n",
      "            \n",
      "                    \n",
      "    #reset the collector once complete            \n",
      "    collector.start_time = start_time\n",
      "    collector.end_time = end_time    \n",
      "    return station_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 256
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pydap.client import open_url"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 257
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_model_data(dap_urls,st_lat,st_lon):\n",
      "    # use only data within 0.04 degrees (about 4 km)\n",
      "    max_dist=0.04 \n",
      "    \n",
      "    # use only data where the standard deviation of the time series exceeds 0.01 m (1 cm)\n",
      "    # this eliminates flat line model time series that come from land points that \n",
      "    # should have had missing values.\n",
      "    min_var=0.01\n",
      "    for url in dap_urls:\n",
      "        try:\n",
      "           pass            \n",
      "        except:\n",
      "            pass\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 258
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get model data for a station\n",
      "get_model_data(dap_urls,41.138,-72.665)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 259
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Embeds the HTML source of the map directly into the IPython notebook.\n",
      "\n",
      "def inline_map(map):   \n",
      "    map._build_map()\n",
      "    return HTML('<iframe srcdoc=\"{srcdoc}\" style=\"width: 100%; height: 500px; border: none\"></iframe>'.format(srcdoc=map.HTML.replace('\"', '&quot;')))\n",
      "\n",
      "map = folium.Map(location=[bounding_box[0][1], bounding_box[0][0]], zoom_start=6)\n",
      "\n",
      "station_yearly_max = []\n",
      "for s in station_list:\n",
      "    #get the station data from the sos end point\n",
      "    if s[\"type\"] is \"obs\":\n",
      "        #get the long name        \n",
      "        station_num = str(s['station_id']).split(':')[-1]\n",
      "        s[\"station_num\"] = station_num\n",
      "        s[\"long_name\"] = get_station_long_name(s['station_id'])\n",
      "        raw_data = get_sos_data(collector,station_num,\"waves\",\"date_time\",\"sea_surface_wave_significant_height (m)\")    \n",
      "        s[\"data\"] = raw_data\n",
      "    if \"latitude\" in s:\n",
      "        popup_string = '<b>Station:</b><br>'+str(s['station_id']) + \"<br><b>Long Name:</b><br>\"+str(s[\"long_name\"])+\"<br><br>\"+str(s[\"type\"])\n",
      "        map.simple_marker([s[\"latitude\"],s[\"longitude\"]],popup=popup_string)\n",
      "# Create the map and add the bounding box line\n",
      "map.line(get_coordinates(bounding_box,bounding_box_type), line_color='#FF0000', line_weight=5)\n",
      "\n",
      "inline_map(map)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import prettyplotlib as ppl\n",
      "fig, ax = plt.subplots(1)\n",
      "\n",
      "# Show the whole color range\n",
      "for s in station_list:\n",
      "    if \"data\" in s:\n",
      "        years = s[\"data\"].keys()\n",
      "        xx = []\n",
      "        yx = []\n",
      "        for y in years:                \n",
      "            val = s[\"data\"][y][\"max\"]            \n",
      "            if val is not None:\n",
      "                try:\n",
      "                    #round to 2dp                    \n",
      "                    val = \"%.2f\" % val\n",
      "                    yx.append(val)\n",
      "                    xx.append(int(y))\n",
      "                except:\n",
      "                    pass\n",
      "                    \n",
      "        #ppl.scatter(ax, xx, yx,alpha=0.8,edgecolor='black',linewidth=0.15 ,label=str(s[\"station_num\"])+\":\"+str(s[\"long_name\"][0]))\n",
      "        ppl.scatter(ax, xx, yx,alpha=0.8,edgecolor='black',linewidth=0.15 ,label=str(s[\"long_name\"]))  \n",
      "     \n",
      "        \n",
      "ax.legend(loc=1)\n",
      "ax.set_title('Annual Max sea surface wave significant height (m) (Observed & Model)')\n",
      "ax.set_xlabel('Year')\n",
      "ax.set_ylabel('sea surface wave significant height (m)')\n",
      "\n",
      "ax.set_xticks(numpy.arange(st_yr,ed_yr,2))\n",
      "fig.set_size_inches(14,8)\n",
      "\n",
      "# Shink current axis by 20%\n",
      "box = ax.get_position()\n",
      "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
      "# Put a legend to the right of the current axis\n",
      "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Extreme Value Analysis:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "annual_max = yx\n",
      "data_levels = []\n",
      "for i in annual_max:\n",
      "    data_levels.append(float(i))\n",
      "annual_max = data_levels    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Fit data to GEV distribution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gev_pdf(x):\n",
      "    return genextreme.pdf(x, xi, loc=mu, scale=sigma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mle = genextreme.fit(sorted(annual_max), 0)\n",
      "mu = mle[1]\n",
      "sigma = mle[2]\n",
      "xi = mle[0]\n",
      "print \"The mean, sigma, and shape parameters are %s, %s, and %s, resp.\" % (mu, sigma, xi)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Probability Density Plot"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_x = min(annual_max_levels)-0.5\n",
      "max_x = max(annual_max_levels)+0.5\n",
      "x = np.linspace(min_x, max_x, num=100)\n",
      "y = [gev_pdf(z) for z in x]\n",
      "\n",
      "fig = plt.figure(figsize=(12,6))\n",
      "axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
      "xlabel = (s[\"long_name\"] + \" - Annual max Wave Height (m)\")\n",
      "axes.set_title(\"Probability Density & Normalized Histogram\")\n",
      "axes.set_xlabel(xlabel)\n",
      "axes.plot(x, y, color='Red')\n",
      "axes.hist(annual_max_levels, bins=arange(min_x, max_x, abs((max_x-min_x)/10)), normed=1, color='Yellow')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Return Value Plot"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(20,6))\n",
      "axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
      "T=np.r_[1:500]\n",
      "sT = genextreme.isf(1./T, 0, mu, sigma)\n",
      "axes.semilogx(T, sT, 'r'), hold\n",
      "N=np.r_[1:len(annual_max_levels)+1]; \n",
      "Nmax=max(N);\n",
      "axes.plot(Nmax/N, sorted(annual_max_levels)[::-1], 'bo')\n",
      "title = s[\"long_name\"][0] \n",
      "axes.set_title(title)\n",
      "axes.set_xlabel('Return Period (yrs)')\n",
      "axes.set_ylabel('Return Value') \n",
      "axes.grid(True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Compute Confidence Intervals"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def conf_int_scipy(x, ci=0.95):\n",
      "  low_per = 100*(1-ci)/2.\n",
      "  high_per = 100*ci + low_per\n",
      "  mn = x.mean()\n",
      "  cis = ss.scoreatpercentile(x, low_per, high_per)\n",
      "  return mn, cis"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}